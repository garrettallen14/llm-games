# Games are the future of AI evaluation. 
*(AKA: Once an LLM can play LoL, then our work here is done.)*

Our benchmarks are becoming saturated because we're measuring the wrong things. Consider why humans created games in the first place: they provide environments where intelligence, strategy, and ethics naturally co-evolve.

When a basketball coach develops a new defensive scheme, they're not just solving a static problem - they're participating in an evolutionary process. If a strategy works, it spreads. Others adapt, counter-strategies emerge, the meta shifts. Through this process, we observe something profound: in every competitive environment, agents face choices between pure optimization and environmental preservation (https://en.m.wikipedia.org/wiki/Instrumental_convergence).

Consider what happens when a strategy emerges that could win games but would destroy what makes the game valuable - constant fouling in basketball, spawn-camping in video games, or exploitation of game-breaking bugs. The emergence of sportsmanship isn't arbitrary - it's a measurable signal of whether an agent will optimize purely for victory or maintain the conditions that make the environment valuable for development. This distinction is precisely what we need to measure in AI systems: not just their capabilities, but their tendency to use those capabilities in ways that preserve the environment rather than ruining the game for everyone else.

This is what our static benchmarks miss entirely. They're like measuring a basketball team's ability by having them shoot free throws. Sure, it's quantifiable, but it captures nothing of the dynamic intelligence that emerges in competition. The rapid saturation of benchmarks isn't showing us progress - it's showing us the inherent deficiencies in current evaluation frameworks.

Games provide natural environments where capability, strategy, and ethics aren't separate metrics to be measured, but intertwined aspects of intelligence that emerge under competitive pressure. This is what we need for AI evaluation: not better static benchmarks, but dynamic environments that naturally evolve with the capabilities we're trying to measure and understand.

The question isn't how to build better benchmarks. The question is whether we want to measure AI capabilities as they truly are - dynamic, adaptive, and fundamentally interconnected - or continue pursuing metrics that will inevitably be optimized into meaninglessness.

Consider League of Legends: a game demanding seamless integration of strategic thinking, real-time decision making, team coordination, resource management, and meta-game adaptation. When an LLM (or another general LLM-adjacent architecture) masters this level of game playing complexity - and it will - we'll have reached a profound milestone not just in gaming, but in artificial general intelligence. The question is: will we have the right evaluation frameworks in place to understand what this mastery truly means?

This approach becomes even more crucial as major AI labs actively seek external safety evaluation (https://openai.com/index/early-access-for-safety-testing/). These labs recognize they cannot provide oversight for themselves. Games provide naturally decentralized evaluation environments where alignment - or its absence - emerges through observable behavior rather than controlled tests.

The rapid saturation of current benchmarks shows we're approaching these capabilities faster than we're developing ways to properly measure them. We need evaluation frameworks that can evolve alongside AI capabilities - frameworks that can measure not just static performance, but dynamic intelligence as it emerges.

The future of AI evaluation isn't in better benchmarks - it's in better arenas.